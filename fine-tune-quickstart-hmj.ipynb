{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7510a26c-fc17-41c6-816d-270548b0cf91",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers 微调训练入门\n",
    "\n",
    "本示例将介绍基于 Transformers 实现模型微调训练的主要流程，包括：\n",
    "- 数据集下载\n",
    "- 数据预处理\n",
    "- 训练超参数配置\n",
    "- 训练评估指标设置\n",
    "- 训练器基本介绍\n",
    "- 实战训练\n",
    "- 模型保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c539a770-73a0-4af9-b0cf-6df8f2d8cd52",
   "metadata": {},
   "source": [
    "## YelpReviewFull 数据集\n",
    "\n",
    "**Hugging Face 数据集：[ YelpReviewFull ](https://huggingface.co/datasets/yelp_review_full)**\n",
    "\n",
    "### 数据集摘要\n",
    "\n",
    "Yelp评论数据集包括来自Yelp的评论。它是从Yelp Dataset Challenge 2015数据中提取的。\n",
    "\n",
    "### 支持的任务和排行榜\n",
    "文本分类、情感分类：该数据集主要用于文本分类：给定文本，预测情感。\n",
    "\n",
    "### 语言\n",
    "这些评论主要以英语编写。\n",
    "\n",
    "### 数据集结构\n",
    "\n",
    "#### 数据实例\n",
    "一个典型的数据点包括文本和相应的标签。\n",
    "\n",
    "来自YelpReviewFull测试集的示例如下：\n",
    "\n",
    "```json\n",
    "{\n",
    "    'label': 0,\n",
    "    'text': 'I got \\'new\\' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\\\nI took the tire over to Flynn\\'s and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he\\'d give me a new tire \\\\\"this time\\\\\". \\\\nI will never go back to Flynn\\'s b/c of the way this guy treated me and the simple fact that they gave me a used tire!'\n",
    "}\n",
    "```\n",
    "\n",
    "#### 数据字段\n",
    "\n",
    "- 'text': 评论文本使用双引号（\"）转义，任何内部双引号都通过2个双引号（\"\"）转义。换行符使用反斜杠后跟一个 \"n\" 字符转义，即 \"\\n\"。\n",
    "- 'label': 对应于评论的分数（介于1和5之间）。\n",
    "\n",
    "#### 数据拆分\n",
    "\n",
    "Yelp评论完整星级数据集是通过随机选取每个1到5星评论的130,000个训练样本和10,000个测试样本构建的。总共有650,000个训练样本和50,000个测试样本。\n",
    "\n",
    "## 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac0e3f46-f937-4b7b-841e-98e7cfeda532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\miniconda3\\envs\\hmj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc14c4bd-1835-4cdf-9e66-24e40b842ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15aeeec0-5902-4c22-a14a-819769fcdf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd349891-84d0-469d-8b2d-d655e4b23943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b705cb71-55a8-428e-88e4-ca172e03730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a69620-322d-41d8-97f4-8ac39063370a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 star</td>\n",
       "      <td>This business is closed now.  This business is closed now. This business is closed now. This business is closed now. This business is closed now. This business is closed now. This business is closed now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>This location is a little different than the Verizon stores I'm used to. No check-in counter, no list of people waiting so you can see where you stand, you have to have an employee check you in, and then they will call you when it is your turn. \\n\\nThe sales staff on the floor seemed disinterested in fixing our tech support issue, instead wanting to instead sell us an upgrade. The sales manager, though, was more than helpful! He completely solved our problem, and turned two unhappy customers into satisfied ones.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 star</td>\n",
       "      <td>Really good shawarma, Horrible service.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 star</td>\n",
       "      <td>I would give it a zero if I could! We wanted to get some pizza before we headed out to the clubs so we ordered in and got it delivered. we ordered 3 pizzas and it was quite expensive so I was expecting some darn good pizza. however it definitely fell short. it was horrible! cold, crust was hard, all the toppings would fall off. everything was just horrible. I don't see what people like about their pizza because dominos tastes better than them!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>Cool place, good music and great sandwiches at reasonable prices.  Was a regular at the one in Boone and while this one seems a little, um, fake I guess it's still okay.  The Ricochet Rabbit is one of my favorite sandwiches.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>A few of my Spanish friends were grilling out last weekend and we had to pick up some meat, charcoal, and veggies to cook.  I headed in the direction of Food Lion and Jose navigated me to The Atlantic Farmers Market.  It is mostly a Spanish grocery, but it is so worth checking out.  If I ever wanted to play Mexican for a night I would definitely go here for tortillas, avocado, and salsa.  It is so authentic.  When we arrived at the deli, Jose began to tell the butcher exactly what we wanted in Spanish, but just to see, I asked for a few things in English.  Turns out most of the bilingual so there is no real language gap.  I really enjoyed this culturally enhancing experience and so will you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1 star</td>\n",
       "      <td>Seriously this is one of the worst Asian food restaurant ever. We were craving pho nearby our hotel so we went despite the fairly positive reviews, but we were so wrong. I had a order of egg rolls which tasted and smelled wrong. The oil they fried these in must've been bad or old. I had never tasted anything that horrible before. I almost wanted to leave the place but waited for my beef pho which I must admit was the worst ever! The soup was bland, the beef smelled and tasted old, there was only green onions as garnish, no cilantro, bean sprouts, basil, whatsoever! My friends ordered Pat Thai, Kung Pao Chicken, Black a Bean Chicken, etc. but they were all just horrible! The only thing that tasted good was the Sriracha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>Hobby Lobby is every crafter's fantasy! Like others have mentioned, this place has it all! It's hard to not spend hours upon hours browsing their inventory imagining what you can create next. \\n\\nI took my niece here and we had a blast picking out things to make together. I was very impressed with this location because it was well organized, clean, and the aisles were roomy. I was equally impressed with their fabric selection. \\n\\nThey have great sales each week! The only drawback is that they still hand key every item. Having said that, they do a good job training their employees to type fast and to be knowledgeable on the weekly deals. The checkout process didn't take too long, like other Hobby Lobby's I've been to.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>Love love Nordstorm Cafe. Fairly cheap prices for quality food. But man does it get busy.\\n\\nThere's a fairly wide variety of things to choose from, from 8-10 salads, 3 pizza options, to 3 pasta options, and 5-6 sandwich items, as well as a variety of desserts to chose from. \\n\\nFor bargain deals, get the grilled cheese and tomato soup combo (its listed as for kids but they don't care if adults order it :P) The tomato soup is DELICIOUS. It's just so yummy and a perfect pair for the grilled cheese. \\n\\nFor salads, I personally love the mixed berry and chicken as well as the blue cheese and pear salad. Everything is made to order. \\n\\nI don't particularly care for the pizzas, but the wild mushroom pasta is delicious, with a hefty portion! \\n\\nAfter a long day of shopping and browsing through this huge mall, Nordstorm Cafe is where quality meets quantity!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>I saw a sign for this place on our way to the movies this weekend. We stopped by for lunch after our movie.\\n\\nPlace is small and so so. Service was really good.\\nWe had the humus with meat (great idea), hubby had a gyro sandwich (pita wrap) and he loved it. Fries were good too, tasty and crunchy.\\n\\nI had ordered a cubideh sandwich (wrap) but ended up just eating the humus, and taking the wrap home.\\n\\nHubby thought humus was kind of bland, I thought it was OK.  The meat was like hamburger meat but with herbs and spices, I liked it very much. The portion was reasonable (~$8, it is an appetizer).\\n\\nWe want to come back to try the grape leaves, the baba ganoush, and the tabouleh.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3894d8-187c-4a17-90b2-2d82866f2674",
   "metadata": {},
   "source": [
    "## 预处理数据\n",
    "\n",
    "下载数据集到本地后，使用 Tokenizer 来处理文本，对于长度不等的输入数据，可以使用填充（padding）和截断（truncation）策略来处理。\n",
    "\n",
    "Datasets 的 `map` 方法，支持一次性在整个数据集上应用预处理函数。\n",
    "\n",
    "下面使用填充到最大长度的策略，处理整个数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579e5533-8d8d-41a3-8130-e8f49cacb5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\miniconda3\\envs\\hmj\\Lib\\site-packages\\transformers\\utils\\hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f56dbb51-dc9b-4ce9-a687-d0c7d99f9ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af5c159c-901d-4c61-b965-0d3601ca51ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>Love the place!! This exactly whats needed after a long day exploring the strip &amp; shopping. \\nStaff are super friendly &amp; overall the place is aesthetically pleasing. Usually pretty busy so you may end up waiting a while but the foods worth it. \\n\\nGot some buffalo chicken strips &amp; guh... Big portions, yet so damned tasty ... &amp; I still don't like celery regardless of what its dipped in. \\nRefreshing smoothies to boot with care taken in regards to allergies. \\nTried the vanilla bean cheesecake, it was lovely &amp; light. I'm gutted we didn't get to try all of the cakes the bakery looked amazing!!!!! \\nReasonably priced as well &amp; overall pleasant experience, so I have no complaints asides from potentially long waits.</td>\n",
       "      <td>[101, 2185, 1103, 1282, 106, 106, 1188, 2839, 1184, 1116, 1834, 1170, 170, 1263, 1285, 12138, 1103, 6322, 111, 6001, 119, 165, 183, 1708, 1777, 3101, 1132, 7688, 4931, 111, 2905, 1103, 1282, 1110, 13492, 2716, 16150, 4253, 119, 12378, 2785, 5116, 1177, 1128, 1336, 1322, 1146, 2613, 170, 1229, 1133, 1103, 11785, 3869, 1122, 119, 165, 183, 165, 183, 2349, 3329, 1199, 26700, 9323, 13507, 111, 176, 23698, 119, 119, 119, 2562, 8924, 117, 1870, 1177, 10688, 27629, 13913, 119, 119, 119, 111, 146, 1253, 1274, 112, 189, 1176, 172, 11194, 1616, 8334, 1104, 1184, 1157, 13916, 1107, 119, ...]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(tokenized_datasets[\"train\"], num_examples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f901a7-2501-4f4e-81e0-85f5718b5272",
   "metadata": {},
   "source": [
    "### 数据抽样\n",
    "\n",
    "使用 1000 个数据样本，在 BERT 上演示小规模训练（基于 Pytorch Trainer）\n",
    "\n",
    "`shuffle()`函数会随机重新排列列的值。如果您希望对用于洗牌数据集的算法有更多控制，可以在此函数中指定generator参数来使用不同的numpy.random.Generator。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cb388e2-e257-4020-96d6-b4e43c6aa02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddfd013-60b2-4244-bea3-713087587133",
   "metadata": {},
   "source": [
    "## 微调训练配置\n",
    "\n",
    "### 加载 BERT 模型\n",
    "\n",
    "警告通知我们正在丢弃一些权重（`vocab_transform` 和 `vocab_layer_norm` 层），并随机初始化其他一些权重（`pre_classifier` 和 `classifier` 层）。在微调模型情况下是绝对正常的，因为我们正在删除用于预训练模型的掩码语言建模任务的头部，并用一个新的头部替换它，对于这个新头部，我们没有预训练的权重，所以库会警告我们在用它进行推理之前应该对这个模型进行微调，而这正是我们要做的事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "948a606b-26b5-4c05-a293-7a4d6a998f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|█████████████████████████████████████████████████████████████| 436M/436M [08:22<00:00, 867kB/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d56ae84-0b7d-4f18-9571-8b69a3559c88",
   "metadata": {},
   "source": [
    "### 训练超参数（TrainingArguments）\n",
    "\n",
    "完整配置参数与默认值：https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "\n",
    "源代码定义：https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161\n",
    "\n",
    "**最重要配置：模型权重保存路径(output_dir)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "474e47a7-ed42-491b-a486-fe6d0068531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "model_dir = \"models/bert-base-cased\"\n",
    "\n",
    "# logging_steps 默认值为500，根据我们的训练数据和步长，将其设置为100\n",
    "training_args = TrainingArguments(output_dir=f\"{model_dir}/test_trainer\",\n",
    "                                  logging_dir=f\"{model_dir}/test_trainer/runs\",\n",
    "                                  logging_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48a817c0-fd1c-438f-ad02-1fd6306afaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/bert-base-cased/test_trainer/runs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "output_dir=models/bert-base-cased/test_trainer,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=models/bert-base-cased/test_trainer,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 完整的超参数配置\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c113020-4e96-4b67-a1be-5ae2c5325bab",
   "metadata": {},
   "source": [
    "### 训练过程中的指标评估（Evaluate)\n",
    "\n",
    "**[Hugging Face Evaluate 库](https://huggingface.co/docs/evaluate/index)** 支持使用一行代码，获得数十种不同领域（自然语言处理、计算机视觉、强化学习等）的评估方法。 当前支持 **完整评估指标：https://huggingface.co/evaluate-metric**\n",
    "\n",
    "训练器（Trainer）在训练过程中不会自动评估模型性能。因此，我们需要向训练器传递一个函数来计算和报告指标。 \n",
    "\n",
    "Evaluate库提供了一个简单的准确率函数，您可以使用`evaluate.load`函数加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f1e9448-47ec-4f16-b4f6-3b2b8c14104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc42ef-772d-4a19-aeb8-6d2fb0806ca7",
   "metadata": {},
   "source": [
    "\n",
    "接着，调用 `compute` 函数来计算预测的准确率。\n",
    "\n",
    "在将预测传递给 compute 函数之前，我们需要将 logits 转换为预测值（**所有Transformers 模型都返回 logits**）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d0639e7-d679-4fc8-a7e4-9c472de10640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f51c91f-a482-4a82-a12a-10d3216e60e8",
   "metadata": {},
   "source": [
    "#### 训练过程指标监控\n",
    "\n",
    "通常，为了监控训练过程中的评估指标变化，我们可以在`TrainingArguments`指定`evaluation_strategy`参数，以便在 epoch 结束时报告评估指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e7eb4ea-1e19-43eb-9d3b-050bab9df823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "#training_args = TrainingArguments(output_dir=f\"{model_dir}/test_trainer\", evaluation_strategy=\"epoch\")\n",
    "\n",
    "training_args = TrainingArguments(output_dir=f\"{model_dir}/test_trainer\",\n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  logging_dir=f\"{model_dir}/test_trainer/runs\",\n",
    "                                  logging_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c14ea-2a02-48fd-9174-31fa43f4c8b2",
   "metadata": {},
   "source": [
    "## 开始训练\n",
    "\n",
    "### 实例化训练器（Trainer）\n",
    "\n",
    "`kernel version` 版本问题：暂不影响本示例代码运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9473272d-1d97-40f1-94e3-784c4fd41bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3af6fff-45f6-4a5d-bf0e-025200d9b7e8",
   "metadata": {},
   "source": [
    "## 使用 nvidia-smi 查看 GPU 使用\n",
    "\n",
    "为了实时查看GPU使用情况，可以使用 `watch` 指令实现轮询：`watch -n 1 nvidia-smi`:\n",
    "\n",
    "```shell\n",
    "Every 1.0s: nvidia-smi                                                   Wed Dec 20 14:37:41 2023\n",
    "\n",
    "Wed Dec 20 14:37:41 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   64C    P0              69W /  70W |   6665MiB / 15360MiB |     98%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     18395      C   /root/miniconda3/bin/python                6660MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8969f91d-9f06-4ad4-a258-197b9cdf402c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 02:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.412400</td>\n",
       "      <td>1.149103</td>\n",
       "      <td>0.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.063300</td>\n",
       "      <td>1.057826</td>\n",
       "      <td>0.545000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.826600</td>\n",
       "      <td>1.021762</td>\n",
       "      <td>0.579000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=1.0267659200032553, metrics={'train_runtime': 167.8596, 'train_samples_per_second': 17.872, 'train_steps_per_second': 2.234, 'total_flos': 789354427392000.0, 'train_loss': 1.0267659200032553, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5ad9ed7-7357-43f2-b310-bbee27f541bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=64).select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2344e01e-60c9-4308-ab4a-b5f5771a3c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0513006448745728,\n",
       " 'eval_accuracy': 0.51,\n",
       " 'eval_runtime': 3.503,\n",
       " 'eval_samples_per_second': 57.095,\n",
       " 'eval_steps_per_second': 7.137,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(small_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544ecf0b-d7a4-40f1-befa-801e22e0196b",
   "metadata": {},
   "source": [
    "### 保存模型和训练状态\n",
    "\n",
    "- 使用 `trainer.save_model` 方法保存模型，后续可以通过 from_pretrained() 方法重新加载\n",
    "- 使用 `trainer.save_state` 方法保存训练状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f266119-0913-45dd-bdf9-5ca6e9ed62bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f\"{model_dir}/finetuned-trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d079e149-44e8-46bb-90c0-e64f14bd2de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b89226-5697-4202-8032-bba541201303",
   "metadata": {},
   "source": [
    "家庭作业-HOMEWork "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30011133-fb9c-4271-ac0a-7df5b30c2d85",
   "metadata": {},
   "source": [
    "#以下全数据量训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c3d0f4-9a99-434d-810e-ab7c3631a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "small_train_dataset2 = tokenized_datasets[\"train\"].shuffle(seed=36)\n",
    "small_eval_dataset2 = tokenized_datasets[\"test\"].shuffle(seed=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09126337-2ff8-43f8-8509-eceb720c7c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(\"models/bert-base-cased/finetuned-trainer\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a12e22e6-432a-42ae-9e89-a6a5e046401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\miniconda3\\envs\\hmj\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_dir = \"models/bert-base-cased\"\n",
    "\n",
    "'''\n",
    "evaluation_strategy, 有三种取值\n",
    "    “no”, 默认取值, 训练期间不 eval\n",
    "    “steps”, 按照另一个参数 “eval_steps” 指定的步数间隔作评估\n",
    "    “epoch”, 每个 epoch 结束做一次评估\n",
    "'''\n",
    "\n",
    "training_args2 = TrainingArguments(output_dir=f\"{model_dir}/test_trainer2\",\n",
    "                                   per_device_eval_batch_size=50,per_device_train_batch_size=13,\n",
    "                                  evaluation_strategy=\"steps\",eval_steps=10000,\n",
    "                                  #evaluation_strategy=\"epoch\",\n",
    "                                  num_train_epochs=2.0,\n",
    "                                  save_strategy=\"steps\",save_steps=10000,\n",
    "                                  logging_dir=f\"{model_dir}/test_trainer2/runs\",\n",
    "                                  logging_steps=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ff87fe9-ca72-45a1-8e7c-b708e234bac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=10000,\n",
      "evaluation_strategy=IntervalStrategy.STEPS,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/bert-base-cased/test_trainer2/runs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "output_dir=models/bert-base-cased/test_trainer2,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=50,\n",
      "per_device_train_batch_size=13,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=models/bert-base-cased/test_trainer2,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=10000,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(training_args2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3237aa7f-2f07-4cc5-94dd-4ee330cb9dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#验证方法\n",
    "import numpy as np\n",
    "import evaluate\n",
    "metric2 = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics2(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric2.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a958a201-7756-4970-9581-79ab09e9aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2 = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args2,\n",
    "    train_dataset=small_train_dataset2,\n",
    "    eval_dataset=small_eval_dataset2,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4132f94e-3714-4746-9592-116d540eb849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100000' max='100000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100000/100000 15:10:02, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.865600</td>\n",
       "      <td>0.822764</td>\n",
       "      <td>0.641620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.826100</td>\n",
       "      <td>0.811897</td>\n",
       "      <td>0.643180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.791100</td>\n",
       "      <td>0.775904</td>\n",
       "      <td>0.662640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.757300</td>\n",
       "      <td>0.801821</td>\n",
       "      <td>0.656400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.777700</td>\n",
       "      <td>0.757012</td>\n",
       "      <td>0.668260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.716200</td>\n",
       "      <td>0.755280</td>\n",
       "      <td>0.675660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.711100</td>\n",
       "      <td>0.755496</td>\n",
       "      <td>0.675920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.697500</td>\n",
       "      <td>0.727333</td>\n",
       "      <td>0.685560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.678200</td>\n",
       "      <td>0.721342</td>\n",
       "      <td>0.689740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>0.715413</td>\n",
       "      <td>0.691060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100000, training_loss=0.7596662670898437, metrics={'train_runtime': 54603.2387, 'train_samples_per_second': 23.808, 'train_steps_per_second': 1.831, 'total_flos': 3.420535852032e+17, 'train_loss': 0.7596662670898437, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#训练开始\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad10405a-5673-4beb-ab99-830cdf4fcd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7575817704200745,\n",
       " 'eval_accuracy': 0.685,\n",
       " 'eval_runtime': 2.9125,\n",
       " 'eval_samples_per_second': 68.67,\n",
       " 'eval_steps_per_second': 1.373,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#最终验证\n",
    "small_test_dataset2 = tokenized_datasets[\"test\"].shuffle(seed=64).select(range(200))\n",
    "trainer2.evaluate(small_test_dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d13c4e1a-4a96-4b72-a751-8eae0d288d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型保存\n",
    "trainer2.save_model(f\"{model_dir}/finetuned-trainer2/checkpoint-end/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
